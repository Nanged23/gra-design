import os
import re
from pypinyin import lazy_pinyin
from src.cur_platform.article.service.ai_service import get_summary
from src.cur_platform.article.entity import Article
from flask import jsonify, request, after_this_request
from src.basic.extensions import db, executor
import oss2
from oss2.credentials import EnvironmentVariableCredentialsProvider
import uuid
from dotenv import load_dotenv
from src.user.utils.add_score import add_score
from src.basic.extensions import redis_client
from collections import defaultdict
from sqlalchemy import and_
from sqlalchemy.sql.expression import func

load_dotenv()

word_count_key = f"word_count"


def handle_word_diff(word_diff, user_id):
    try:
        redis_client.hincrby(word_count_key, user_id, word_diff)
        print(f"ç”¨æˆ·{user_id}æ–‡ç« æ€»å­—æ•°å˜æ›´äº†{word_diff}ã€‚")
    except Exception as e:
        print(f"Redis æ“ä½œå¤±è´¥ï¼š{e}")


def delete_cover(relative_url):
    bucket = init_oss()
    bucket.delete_object(relative_url)


def upload_cover(cover):
    bucket = init_oss()
    filename = cover.filename
    ext = filename.rsplit('.', 1)[1].lower()  # è·å–æ–‡ä»¶åç¼€å
    new_filename = str(uuid.uuid4()) + '.' + ext  # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
    object_name = f'æ–‡ç« å°é¢/{new_filename}'
    bucket.put_object(object_name, cover.read())
    url = "https://" + os.getenv('OSS_BUCKET_NAME') + ".oss-" + os.getenv(
        'OSS_REGION') + ".aliyuncs.com/" + object_name
    return url


def init_oss():
    """
    åˆå§‹åŒ– oss
    """
    auth = oss2.ProviderAuthV4(EnvironmentVariableCredentialsProvider())
    endpoint = os.getenv('OSS_ENDPOINT')
    region = os.getenv('OSS_REGION')
    bucket = oss2.Bucket(auth, endpoint, os.getenv('OSS_BUCKET_NAME'), region=region)
    return bucket


def generate_slug(title, delimiter='-'):
    """
    ç»™æ–‡ç« ç”Ÿæˆ slug
    :param title:æ ‡é¢˜
    :param delimiter: æ ‡é¢˜é—´è¯å¥åˆ†éš”ç¬¦
    :return:
    """
    # å°†ä¸­æ–‡è½¬æ¢ä¸ºæ‹¼éŸ³
    pinyin_list = lazy_pinyin(title)
    normalized_text = ''.join(pinyin_list)
    stripped_text = re.sub(r'[^a-zA-Z0-9\s_-]', '', normalized_text)
    slug = re.sub(r'\s+', delimiter, stripped_text).strip().lower()
    final_slug = re.sub(r'[-_]+', delimiter, slug)
    return final_slug


def generate_excerpt(content, article_id):
    """
    æ ¹æ® content ç”Ÿæˆæ–‡ç« æ‘˜è¦ï¼Œå¹¶æ‰¾åˆ°å¯¹åº”çš„æ–‡ç« ï¼Œæ›´æ–°å…¶å­—æ®µ
    :param content: åŸæ–‡ç« å†…å®¹
    :param article_id: æ–‡ç«  id
    :return:
    """
    excerpt = get_summary(content)
    Article.query.filter_by(article_id=article_id).update({"excerpt": excerpt})
    db.session.commit()
    return


def write_article(title, content, user_id, tags, cover, word_diff, summary_min_len=500):
    # TODO å¢åŠ æœˆã€å¹´ã€å‘¨çš„æ–‡ç« ä¹¦å†™æŠ¥å‘Š
    if tags is None or tags == '':
        tags = ''  # TODO ä½¿ç”¨ AI ç”Ÿæˆæ ‡ç­¾
    # TODO ä½¿ç”¨æ­£åˆ™åœ¨è‹±æ–‡å•è¯å‰ååŠ ç©ºæ ¼
    # ä¸Šè¿° 2 ä¸ªåŠŸèƒ½ç»Ÿä¸€ä¸º [ AIå¸®å¿™ ]
    if cover is None or cover == '':  # åŠ å…¥é»˜è®¤æ–‡ç« å°é¢
        cover = 'https://guli-college0.oss-cn-chengdu.aliyuncs.com/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2/default_cover.png'
    else:
        cover = upload_cover(cover)
    slug = generate_slug(title)
    excerpt = ''  # ai æ‘˜è¦é»˜è®¤ç½®ç©º,åœ¨å¼‚æ­¥ä»»åŠ¡ä¸­ä¼šè¿›è¡Œå¡«å……
    blog_post = Article(title=title, slug=slug, content=content, excerpt=excerpt,
                        author_id=user_id, tags=tags, cover=cover)
    db.session.add(blog_post)
    db.session.commit()
    article_id = blog_post.id
    add_score(request.full_path, user_id)
    handle_word_diff(word_diff, user_id)
    # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡ï¼Œä½†ä¸ç­‰å¾…å®ƒå®Œæˆ
    if len(content) >= summary_min_len:
        @after_this_request
        def after_request(response):
            executor.submit(generate_excerpt, content, article_id)
            return response

    return jsonify({'msg': 'ğŸ‰ æ­å–œä½ ï¼Œæ–°å†™äº†ä¸€ç¯‡æ–‡ç« ï½'}), 200


def get_article(user_id, type, extra, default_len=20, per_page=5, tag=None):
    """
    :param user_id: ç”¨æˆ· id
    :param type: è¡¨æ˜æ˜¯è·å–æ‰€æœ‰æ–‡ç« è¿˜æ˜¯å•ç¯‡æ–‡ç« 
    :param extra: å½“ type ä¸º 0 æ—¶ï¼Œè¡¨ç¤ºå½“å‰é¡µç ï¼›å½“ type ä¸º 1 æ—¶ï¼Œè¡¨ç¤ºæ–‡ç«  id
    :param default_len:é»˜è®¤ç¼©ç•¥æ—¶ï¼Œæˆªå–çš„é•¿åº¦
    :param per_page:æ¯é¡µå±•ç¤ºæ•°é‡
    :param tag:è·å–æŸä¸€æ ‡ç­¾ä¸‹çš„åˆ†é¡µæ–‡ç« 
    :return:
    """
    if type == '0':  # è·å–å½“å‰é¡µç çš„æ‰€æœ‰æ–‡ç« 
        # æ¯é¡µæ˜¾ç¤ºçš„æ–‡ç« æ•°
        if tag is None:
            articles = Article.query.filter_by(author_id=user_id).paginate(page=int(extra), per_page=per_page)
        else:
            # å°† tags å­—æ®µä¸­çš„é€—å·ç­‰è¿›è¡Œè½¬ä¹‰ï¼Œé˜²æ­¢æ‹¼å‡‘ sql å‡ºé”™è¯¯
            escaped_tag = re.escape(tag)

            # æ„å»ºæ­£åˆ™è¡¨è¾¾å¼
            regex_pattern = rf"(^|,){escaped_tag}(,|$)"
            # æ„å»ºæŸ¥è¯¢
            articles = Article.query.filter(
                and_(
                    Article.author_id == user_id,
                    Article.tags.op('REGEXP')(regex_pattern)
                )
            ).paginate(page=int(extra), per_page=per_page)

        all_articles = [article.to_dict() for article in articles.items]
        for i in all_articles:  # ç¼©ç•¥æ–‡æœ¬ç”¨äºå±•ç¤º
            i['content'] = i['content'][:default_len] + '...' if len(i['content']) > default_len else i['content']
        return jsonify({"msg": "success", "data": {'total_pages': articles.pages, 'total_items': articles.total,
                                                   'items': all_articles}}), 200
    elif type == '1':  # è·å–å•ç¯‡æ–‡ç« è¯¦æƒ…
        article = Article.query.filter_by(id=extra).first()
        article.views_count += 1
        db.session.add(article)  # å°† article å¯¹è±¡æ·»åŠ åˆ° session ä¸­ï¼Œè¡¨ç¤ºè¦æ›´æ–°å®ƒ
        db.session.commit()
        # æŸ¥è¯¢ä¸Šä¸€ç¯‡æ–‡ç« å’Œä¸‹ä¸€ç¯‡æ–‡ç« 
        previous_article = Article.query.filter(Article.id < extra).order_by(Article.id.desc()).first()
        next_article = Article.query.filter(Article.id > extra).order_by(Article.id.asc()).first()
        pre_article = {
            "id": previous_article.id,
            "title": previous_article.title
        } if previous_article else None

        next_article = {
            "id": next_article.id,
            "title": next_article.title
        } if next_article else None
        read_time = len(article.content) // 300
        read_time = 1 if read_time <= 1 else read_time
        return jsonify({"msg": "success",
                        "data": {'read_time': read_time, 'pre_article': pre_article, 'next_article': next_article,
                                 'items': [article.to_dict()]}}), 200
    else:
        return jsonify({'msg': 'å‚æ•°å¼‚å¸¸'}), 400


def delete_article(article_id, word_diff):
    article = Article.query.filter_by(id=article_id).first()
    cover_relative_url = '/'.join(article.cover.split('/')[-2:])
    delete_cover(cover_relative_url)
    user_id = article.author_id
    if article is None:
        return jsonify({'msg': 'æ–‡ç« ä¸å­˜åœ¨~'}), 400
    db.session.delete(article)
    db.session.commit()
    handle_word_diff(word_diff, user_id)
    return jsonify({'msg': 'æ–‡ç« åˆ é™¤æˆåŠŸ~'}), 200


def update_article(article_id, data, cover_file, word_diff):
    article = Article.query.get(article_id)
    cover_relative_url = '/'.join(article.cover.split('/')[-2:])
    if cover_file:
        delete_cover(cover_relative_url)
        article.cover = upload_cover(cover_file)
    for key, value in data.items():
        if hasattr(article, key):
            setattr(article, key, value)
    db.session.commit()
    user_id = article.author_id
    handle_word_diff(word_diff, user_id)
    return jsonify({'msg': 'æ–‡ç« æ›´æ–°æˆåŠŸ'}), 200


def get_word_count(user_id):
    dic = [
        (255, 'å†åˆ«åº·æ¡¥'),
        (800, "æ–°é—»çŸ­è¯„"),
        (2500, "å¥‘è¯ƒå¤«çš„ã€Šå˜è‰²é¾™ã€‹")
    ]

    value = redis_client.hget(word_count_key, user_id)
    value = int(value)
    matched_text = None
    for threshold, title in dic:
        if value <= threshold:
            matched_text = (threshold, title)
            break
        else:
            # ç”¨æˆ·æ–‡ç« å­—æ•°å¤§äºæ‰€æœ‰é˜ˆå€¼ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªå‚è€ƒæ–‡æœ¬
            matched_text = dic[-1]
    threshold, title = matched_text
    multiple = round(value / threshold, 1)
    sentence = f"{multiple} æœ¬{title}"
    return jsonify({'msg': 'success', "data": {'word_count': value, 'sentence': sentence}}), 200


def get_word_cloud(user_id):
    """
    è·å–æ ‡ç­¾è¯äº‘å›¾
    """
    articles = Article.query.filter_by(author_id=user_id).all()
    frequency = defaultdict(int)
    for article in articles:
        tags = article.tags
        for tag in tags.split(","):
            if tag != '':
                frequency[tag] += 1
    return jsonify({"msg": "success", "data": frequency}), 200


def get_by_tags(user_id, extra, tag):
    return get_article(user_id, "0", extra, tag=tag)


def get_most_viewed(user_id, limit=3):
    articles = (
        Article.query.filter_by(author_id=user_id)
        .order_by(Article.views_count.desc())
        .limit(limit)
        .all()
    )
    all_articles = [article.to_dict() for article in articles]
    return jsonify({"msg": "success", "data": all_articles}), 200


def get_time_preference(user_id):
    """åˆ†æç”¨æˆ·å†™ä½œåå¥½æ—¶æ®µï¼ŒæŒ‰å‡Œæ™¨ã€ä¸Šåˆã€ä¸‹åˆã€æ™šä¸Šåˆ’åˆ†"""

    def get_time_slot(hour):
        if 0 <= hour <= 5:
            return "å‡Œæ™¨"
        elif 6 <= hour <= 11:
            return "ä¸Šåˆ"
        elif 12 <= hour <= 17:
            return "ä¸‹åˆ"
        else:
            return "æ™šä¸Š"

    time_slots = (
        db.session.query(
            func.hour(Article.modify_time),
            func.count(Article.id)
        )
        .filter(Article.author_id == user_id)
        .group_by(func.hour(Article.modify_time))
        .all()
    )

    # å°†ç»“æœè½¬æ¢ä¸ºå­—å…¸ï¼Œå¹¶åº”ç”¨æ—¶é—´åŒºé—´åˆ’åˆ†
    result = {}
    for hour, count in time_slots:
        slot = get_time_slot(hour)
        result[slot] = result.get(slot, 0) + count
    print(result)
    return jsonify({"msg": "success", "data": result}), 200
