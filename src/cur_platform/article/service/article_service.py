import os
import re
from pypinyin import lazy_pinyin
from src.cur_platform.article.service.ai_service import get_summary
import asyncio
from src.cur_platform.article.entity import Article
from flask import jsonify, request
from src.basic.database import db
import oss2
from oss2.credentials import EnvironmentVariableCredentialsProvider
import uuid
from dotenv import load_dotenv
from src.user.utils.add_score import add_score
from src.basic.database import redis_client
from collections import defaultdict

load_dotenv()

word_count_key = f"word_count"


def handle_word_diff(word_diff, user_id):
    try:
        redis_client.hincrby(word_count_key, user_id, word_diff)
        print(f"ç”¨æˆ·{user_id}æ–‡ç« æ€»å­—æ•°å˜æ›´äº†{word_diff}ã€‚")
    except Exception as e:
        print(f"Redis æ“ä½œå¤±è´¥ï¼š{e}")


def upload_img(cover):
    auth = oss2.ProviderAuthV4(EnvironmentVariableCredentialsProvider())
    endpoint = os.getenv('OSS_ENDPOINT')
    region = os.getenv('OSS_REGION')
    bucket = oss2.Bucket(auth, endpoint, os.getenv('OSS_BUCKET_NAME'), region=region)

    filename = cover.filename
    ext = filename.rsplit('.', 1)[1].lower()  # è·å–æ–‡ä»¶åç¼€å
    new_filename = str(uuid.uuid4()) + '.' + ext  # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
    object_name = f'æ–‡ç« å°é¢/{new_filename}'
    bucket.put_object(object_name, cover.read())
    url = "https://" + os.getenv('OSS_BUCKET_NAME') + ".oss-" + os.getenv('OSS_REGION') + ".aliyuncs.com/" + object_name
    return url


def generate_slug(title, delimiter='-'):
    """
    ç»™æ–‡ç« ç”Ÿæˆ slug
    :param title:æ ‡é¢˜
    :param delimiter: æ ‡é¢˜é—´è¯å¥åˆ†éš”ç¬¦
    :return:
    """
    # å°†ä¸­æ–‡è½¬æ¢ä¸ºæ‹¼éŸ³
    pinyin_list = lazy_pinyin(title)

    # å°†æ‹¼éŸ³åˆ—è¡¨åˆå¹¶æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¹¶æ­£å¸¸åŒ–
    normalized_text = ''.join(pinyin_list)

    # ç§»é™¤éå­—æ¯æ•°å­—å­—ç¬¦ï¼Œä¿ç•™è¿å­—ç¬¦å’Œä¸‹åˆ’çº¿
    stripped_text = re.sub(r'[^a-zA-Z0-9\s_-]', '', normalized_text)

    # æ›¿æ¢ç©ºæ ¼ä¸ºæŒ‡å®šçš„åˆ†éš”ç¬¦ï¼Œå¹¶å°†ç»“æœè½¬æ¢ä¸ºå°å†™
    slug = re.sub(r'\s+', delimiter, stripped_text).strip().lower()

    # ç§»é™¤å¤šä½™çš„åˆ†éš”ç¬¦
    final_slug = re.sub(r'[-_]+', delimiter, slug)

    return final_slug


def generate_excerpt(blog_id):
    """
    æ ¹æ®æ–‡ç«  id æŸ¥è¯¢æ–‡ç« å†…å®¹ï¼Œç„¶åç”Ÿæˆæ–‡ç« æ‘˜è¦
    :param blog_id: 
    :return: 
    """
    content = Article.query.filter_by(id=blog_id).first().content
    excerpt = get_summary(content)
    return excerpt


def write_article(title, content, user_id, tags, cover, word_diff, summary_min_len=500):
    if tags is None or tags == '':
        tags = ''  # TODO ä½¿ç”¨ AI ç”Ÿæˆæ ‡ç­¾
    if cover is None or cover == '':  # åŠ å…¥é»˜è®¤æ–‡ç« å°é¢
        cover = 'https://guli-college0.oss-cn-chengdu.aliyuncs.com/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2/default_cover.png'
    else:
        cover = upload_img(cover)
    slug = generate_slug(title)
    excerpt = ''  # ai æ‘˜è¦é»˜è®¤ç½®ç©º,åœ¨å¼‚æ­¥ä»»åŠ¡ä¸­ä¼šè¿›è¡Œå¡«å……
    blog_post = Article(title=title, slug=slug, content=content, excerpt=excerpt,
                        author_id=user_id, tags=tags, cover=cover)
    db.session.add(blog_post)
    db.session.commit()
    add_score(request.full_path, user_id)
    handle_word_diff(word_diff, user_id)
    # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡ï¼Œä½†ä¸ç­‰å¾…å®ƒå®Œæˆ
    if len(content) >= summary_min_len:
        asyncio.create_task(generate_excerpt(blog_post.id))
    return jsonify({'msg': 'ğŸ‰ æ­å–œä½ ï¼Œæ–°å†™äº†ä¸€ç¯‡æ–‡ç« ï½'}), 200


def get_article(user_id, type, extra, default_len=20):
    """
    :param user_id: ç”¨æˆ· id
    :param type: è¡¨æ˜æ˜¯è·å–æ‰€æœ‰æ–‡ç« è¿˜æ˜¯å•ç¯‡æ–‡ç« 
    :param extra: å½“ type ä¸º 0 æ—¶ï¼Œè¡¨ç¤ºå½“å‰é¡µç ï¼›å½“ type ä¸º 1 æ—¶ï¼Œè¡¨ç¤ºæ–‡ç«  id
    :param default_len:é»˜è®¤ç¼©ç•¥æ—¶ï¼Œæˆªå–çš„é•¿åº¦
    :return:
    """
    if type == '0':  # è·å–å½“å‰é¡µç çš„æ‰€æœ‰æ–‡ç« 
        per_page = 5  # æ¯é¡µæ˜¾ç¤ºçš„æ–‡ç« æ•°
        articles = Article.query.filter_by(author_id=user_id).paginate(page=int(extra), per_page=per_page)
        all_articles = [article.to_dict() for article in articles.items]
        for i in all_articles:  # ç¼©ç•¥æ–‡æœ¬ç”¨äºå±•ç¤º
            i['content'] = i['content'][:default_len] + '...' if len(i['content']) > default_len else i['content']
        return jsonify({"msg": "success", "data": {'total_pages': articles.pages, 'total_items': articles.total,
                                                   'items': all_articles}}), 200
    elif type == '1':  # è·å–å•ç¯‡æ–‡ç« è¯¦æƒ…
        article = Article.query.filter_by(id=extra).first()
        article.views_count += 1
        db.session.add(article)  # å°† article å¯¹è±¡æ·»åŠ åˆ° session ä¸­ï¼Œè¡¨ç¤ºè¦æ›´æ–°å®ƒ
        db.session.commit()
        # æŸ¥è¯¢ä¸Šä¸€ç¯‡æ–‡ç« å’Œä¸‹ä¸€ç¯‡æ–‡ç« 
        previous_article = Article.query.filter(Article.id < extra).order_by(Article.id.desc()).first()
        next_article = Article.query.filter(Article.id > extra).order_by(Article.id.asc()).first()
        pre_article = {
            "id": previous_article.id,
            "title": previous_article.title
        } if previous_article else None

        next_article = {
            "id": next_article.id,
            "title": next_article.title
        } if next_article else None
        read_time = len(article.content) // 300
        read_time = 1 if read_time <= 1 else read_time
        return jsonify({"msg": "success",
                        "data": {'read_time': read_time, 'pre_article': pre_article, 'next_article': next_article,
                                 'items': [article.to_dict()]}}), 200
    else:
        return jsonify({'msg': 'å‚æ•°å¼‚å¸¸'}), 400


def delete_article(article_id, word_diff):
    article = Article.query.filter_by(id=article_id).first()
    user_id = article.author_id
    if article is None:
        return jsonify({'msg': 'æ–‡ç« ä¸å­˜åœ¨~'}), 400
    db.session.delete(article)
    db.session.commit()
    handle_word_diff(word_diff, user_id)
    return jsonify({'msg': 'æ–‡ç« åˆ é™¤æˆåŠŸ~'}), 200


def update_article(article_id, data, cover_file, word_diff):
    article = Article.query.get(article_id)
    if cover_file:
        # TODO å…ˆå»åˆ é™¤å¯¹åº”çš„æ–‡ç« å°é¢å†æ–°å¢
        article.cover_image = upload_img(cover_file)  # è°ƒç”¨ upload_img å‡½æ•°
    for key, value in data.items():
        if hasattr(article, key):
            setattr(article, key, value)
    db.session.commit()
    user_id = article.author_id
    handle_word_diff(word_diff, user_id)
    return jsonify({'msg': 'æ–‡ç« æ›´æ–°æˆåŠŸ'}), 200


def get_word_count(user_id):
    dic = [
        (255, 'å†åˆ«åº·æ¡¥'),
        (800, "æ–°é—»çŸ­è¯„"),
        (2500, "å¥‘è¯ƒå¤«çš„ã€Šå˜è‰²é¾™ã€‹")
    ]

    value = redis_client.hget(word_count_key, user_id)
    value = int(value)
    matched_text = None
    for threshold, title in dic:
        if value <= threshold:
            matched_text = (threshold, title)
            break
        else:
            # ç”¨æˆ·æ–‡ç« å­—æ•°å¤§äºæ‰€æœ‰é˜ˆå€¼ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªå‚è€ƒæ–‡æœ¬
            matched_text = dic[-1]
    threshold, title = matched_text
    multiple = round(value / threshold, 1)
    sentence = f"{multiple} æœ¬{title}"
    return jsonify({'msg': 'success', "data": {'word_count': value, 'sentence': sentence}}), 200


def get_word_cloud(user_id):
    articles = Article.query.filter_by(author_id=user_id).all()
    frequency = defaultdict(int)
    for article in articles:
        tags = article.tags
        for tag in tags.split(","):
            if tag != '':
                frequency[tag] += 1
    return jsonify({"msg": "success", "data": frequency}), 200
